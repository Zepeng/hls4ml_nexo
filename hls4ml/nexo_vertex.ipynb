{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Convolutional Neural Networks in hls4ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Start with the neccessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-02 09:10:46.098260: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-02 09:10:46.191601: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-03-02 09:10:46.665627: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-02 09:10:46.665674: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-02 09:10:46.665680: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Build the dataset using Tensorflow Dataset\n",
    "\n",
    "In this part we will fetch the trainining, validation and test dataset using Tensorflow Datasets (https://www.tensorflow.org/datasets). We will not use the 'extra' training in order to save time, but you could fetch it by adding `split='train[:90%]+extra'`. We will use the first 90% of the training data for training and the last 10% for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/zel032/software/miniconda3/envs/hls4ml/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-02 09:10:48.649956: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-03-02 09:10:48.649981: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-03-02 09:10:48.650250: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "input_shape = [200, 255, 2]\n",
    "batch_size = 60\n",
    "from data_loader import VertexDataset\n",
    "from tensorflow.data import Dataset\n",
    "csv_train = '/scratch/zel032/vertex/nexo_train.csv'\n",
    "csv_val = '/scratch/zel032/vertex/nexo_valid.csv'\n",
    "h5file = '/scratch/zel032/vertex/nexo.h5'\n",
    "# load dataset\n",
    "train_dg = VertexDataset('train',h5file,csv_train)\n",
    "train_ds = Dataset.from_generator(train_dg, output_types = (tf.float32, tf.float32) , output_shapes = (tf.TensorShape(input_shape),tf.TensorShape([2])))\n",
    "train_ds = train_ds.interleave(lambda x, y: tf.data.Dataset.from_tensors((x,y)), cycle_length=4, block_length=16)\n",
    "val_dg = VertexDataset('validation',h5file,csv_val)\n",
    "val_ds = Dataset.from_generator(val_dg, output_types = (tf.float32, tf.float32) , output_shapes = (tf.TensorShape(input_shape),tf.TensorShape([2])))\n",
    "val_ds = val_ds.interleave(lambda x, y: tf.data.Dataset.from_tensors((x,y)), cycle_length=4, block_length=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We'll use TensorFlow Dataset to prepare our datasets. We'll fetch the training dataset as tuples, and the test dataset as numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train batch shape = (100, 200, 255, 2), Y train batch shape = (100, 2) \n",
      "X test batch shape = (300, 200, 255, 2), Y test batch shape = (300, 2) \n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "#Get dataset as image and one-hot encoded labels, divided by max RGB   \n",
    "train_data = train_ds.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "for example in train_data.take(1):\n",
    "    break\n",
    "print(\"X train batch shape = {}, Y train batch shape = {} \".format(example[0].shape, example[1].shape))\n",
    " \n",
    "val_data = val_ds.batch(300)\n",
    "val_data = val_data.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# For  testing, we get the full dataset in memory as it's rather small.\n",
    "# We fetch it as numpy arrays to have access to labels and images separately\n",
    "for example in val_data.take(1):\n",
    "    break\n",
    "X_test = example[0]\n",
    "Y_test = example[1]\n",
    "#X_test, Y_test = preprocess(X_test, np.array(Y_test),nclasses=n_classes)\n",
    "print(\"X test batch shape = {}, Y test batch shape = {} \".format(X_test.shape,Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Defining the model\n",
    "\n",
    "We then need to define a model. For the lowest possible latency, each layer should have a maximum number of trainable parameters of 4096. This is due to fixed limits in the Vivado compiler, beyond which maximally unrolled (=parallel) compilation will fail. This will allow us to use `strategy = 'latency'` in the hls4ml part, rather than `strategy = 'resource'`, in turn resulting in lower latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding convolutional block 0 with N=16 filters\n",
      "Adding convolutional block 1 with N=16 filters\n",
      "Adding convolutional block 2 with N=24 filters\n",
      "Adding dense block 0 with N=42 neurons\n",
      "Adding dense block 1 with N=64 neurons\n",
      "Model: \"keras_baseline\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 200, 255, 2)]     0         \n",
      "                                                                 \n",
      " conv_0 (Conv2D)             (None, 198, 253, 16)      288       \n",
      "                                                                 \n",
      " bn_conv_0 (BatchNormalizati  (None, 198, 253, 16)     64        \n",
      " on)                                                             \n",
      "                                                                 \n",
      " conv_act_0 (Activation)     (None, 198, 253, 16)      0         \n",
      "                                                                 \n",
      " pool_0 (MaxPooling2D)       (None, 99, 126, 16)       0         \n",
      "                                                                 \n",
      " conv_1 (Conv2D)             (None, 97, 124, 16)       2304      \n",
      "                                                                 \n",
      " bn_conv_1 (BatchNormalizati  (None, 97, 124, 16)      64        \n",
      " on)                                                             \n",
      "                                                                 \n",
      " conv_act_1 (Activation)     (None, 97, 124, 16)       0         \n",
      "                                                                 \n",
      " pool_1 (MaxPooling2D)       (None, 48, 62, 16)        0         \n",
      "                                                                 \n",
      " conv_2 (Conv2D)             (None, 46, 60, 24)        3456      \n",
      "                                                                 \n",
      " bn_conv_2 (BatchNormalizati  (None, 46, 60, 24)       96        \n",
      " on)                                                             \n",
      "                                                                 \n",
      " conv_act_2 (Activation)     (None, 46, 60, 24)        0         \n",
      "                                                                 \n",
      " pool_2 (MaxPooling2D)       (None, 23, 30, 24)        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 16560)             0         \n",
      "                                                                 \n",
      " dense_0 (Dense)             (None, 42)                695520    \n",
      "                                                                 \n",
      " bn_dense_0 (BatchNormalizat  (None, 42)               168       \n",
      " ion)                                                            \n",
      "                                                                 \n",
      " dense_act_0 (Activation)    (None, 42)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                2688      \n",
      "                                                                 \n",
      " bn_dense_1 (BatchNormalizat  (None, 64)               256       \n",
      " ion)                                                            \n",
      "                                                                 \n",
      " dense_act_1 (Activation)    (None, 64)                0         \n",
      "                                                                 \n",
      " output_dense (Dense)        (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 705,034\n",
      "Trainable params: 704,710\n",
      "Non-trainable params: 324\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "filters_per_conv_layer = [16,16,24]\n",
    "neurons_per_dense_layer = [42,64]\n",
    "\n",
    "x = x_in = Input(input_shape)\n",
    "\n",
    "for i,f in enumerate(filters_per_conv_layer):\n",
    "    print( ('Adding convolutional block {} with N={} filters').format(i,f) )\n",
    "    x = Conv2D(int(f), kernel_size=(3,3), strides=(1,1), kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001), use_bias=False,\n",
    "               name='conv_{}'.format(i))(x) \n",
    "    x = BatchNormalization(name='bn_conv_{}'.format(i))(x)\n",
    "    x = Activation('relu',name='conv_act_%i'%i)(x)\n",
    "    x = MaxPooling2D(pool_size = (2,2),name='pool_{}'.format(i) )(x)\n",
    "x = Flatten()(x)\n",
    "\n",
    "for i,n in enumerate(neurons_per_dense_layer):\n",
    "    print( ('Adding dense block {} with N={} neurons').format(i,n) )\n",
    "    x = Dense(n,kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001),name='dense_%i'%i, use_bias=False)(x)\n",
    "    x = BatchNormalization(name='bn_dense_{}'.format(i))(x)\n",
    "    x = Activation('relu',name='dense_act_%i'%i)(x)\n",
    "x_out = Dense(2,name='output_dense')(x)\n",
    "\n",
    "\n",
    "model = Model(inputs=[x_in], outputs=[x_out], name='keras_baseline')\n",
    "\n",
    "model.summary()\n",
    "import visualkeras\n",
    "visualkeras.layered_view(model).show()\n",
    "visualkeras.layered_view(model, to_file='output.png').show() # write and show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Lets check if this model can be implemented completely unrolled (=parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_0: 288\n",
      "conv_1: 2304\n",
      "conv_2: 3456\n",
      "dense_0: 695520\n",
      "Layer dense_0 is too large (695520), are you sure you want to train?\n",
      "dense_1: 2688\n",
      "output_dense: 128\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    if layer.__class__.__name__ in ['Conv2D', 'Dense']:\n",
    "        w = layer.get_weights()[0]\n",
    "        layersize = np.prod(w.shape)\n",
    "        print(\"{}: {}\".format(layer.name,layersize)) # 0 = weights, 1 = biases\n",
    "        if (layersize > 4096): # assuming that shape[0] is batch, i.e., 'None'\n",
    "            print(\"Layer {} is too large ({}), are you sure you want to train?\".format(layer.name,layersize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Looks good! It's below the Vivado-enforced unroll limit of 4096.\n",
    "\n",
    "## Prune dense and convolutional layers\n",
    "Since we've seen in the previous notebooks that pruning can be done at no accuracy cost, let's prune the convolutional and dense layers to 50% sparsity, skipping the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training steps per epoch is 1800\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "from tensorflow_model_optimization.sparsity import keras as sparsity\n",
    "from tensorflow_model_optimization.python.core.sparsity.keras import pruning_callbacks\n",
    "\n",
    "NSTEPS = int(180000)  // batch_size #90% train, 10% validation in 10-fold cross validation\n",
    "print('Number of training steps per epoch is {}'.format(NSTEPS))\n",
    "\n",
    "# Prune all convolutional and dense layers gradually from 0 to 50% sparsity every 2 epochs, \n",
    "# ending by the 10th epoch\n",
    "def pruneFunction(layer):\n",
    "    pruning_params = {'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity = 0.0,\n",
    "                                                                   final_sparsity = 0.50, \n",
    "                                                                   begin_step = NSTEPS*2, \n",
    "                                                                   end_step = NSTEPS*10, \n",
    "                                                                   frequency = NSTEPS)\n",
    "                     }\n",
    "    if isinstance(layer, tf.keras.layers.Conv2D):\n",
    "        return tfmot.sparsity.keras.prune_low_magnitude(layer, **pruning_params)\n",
    "    if isinstance(layer, tf.keras.layers.Dense) and layer.name!='output_dense':\n",
    "        return tfmot.sparsity.keras.prune_low_magnitude(layer, **pruning_params)  \n",
    "    return layer\n",
    "\n",
    "model_pruned = tf.keras.models.clone_model( model, clone_function=pruneFunction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Train baseline\n",
    "\n",
    "We're now ready to train the model! We defined the batch size and n epochs above. We won't use callbacks that store the best weights only, since this might select a weight configuration that has not yet reached 50% sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = False # True if you want to retrain, false if you want to load a previsously trained model\n",
    "\n",
    "n_epochs = 2\n",
    "\n",
    "if train:\n",
    "    \n",
    "    LOSS        = tf.keras.losses.MeanSquaredError()\n",
    "    OPTIMIZER   = tf.keras.optimizers.Adam(learning_rate=3E-3, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=True)\n",
    "\n",
    "    model_pruned.compile(loss=LOSS, optimizer=OPTIMIZER, metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "\n",
    "    callbacks = [\n",
    "            tf.keras.callbacks.EarlyStopping(patience=10, verbose=1),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1),\n",
    "            pruning_callbacks.UpdatePruningStep()\n",
    "            ] \n",
    "\n",
    "    start = time.time()\n",
    "    model_pruned.fit(train_data,\n",
    "                     epochs = n_epochs,\n",
    "                     validation_data = val_data,\n",
    "                     callbacks = callbacks)   \n",
    "    end = time.time()\n",
    "\n",
    "    print('It took {} minutes to train Keras model'.format( (end - start)/60.))\n",
    "    \n",
    "    model_pruned.save('pruned_vertex_model.h5')\n",
    "\n",
    "else:\n",
    "    from qkeras.utils import _add_supported_quantized_objects\n",
    "    from tensorflow_model_optimization.python.core.sparsity.keras import pruning_wrapper\n",
    "    \n",
    "    co = {}\n",
    "    _add_supported_quantized_objects(co)\n",
    "    co['PruneLowMagnitude'] = pruning_wrapper.PruneLowMagnitude\n",
    "    model_pruned = tf.keras.models.load_model('pruned_vertex_model.h5', custom_objects=co)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Quantization and the fused Conv2D+BatchNormalization layer in QKeras\n",
    "Let's now create a pruned an quantized model using QKeras. For this, we will use a fused Convolutional and BatchNormalization (BN) layer from QKeras, which will further speed up the implementation when we implement the model using hls4ml. \n",
    "There is currently no fused Dense+BatchNoralization layer available in QKeras, so we'll use Keras BatchNormalization when BN follows a Dense layer for now. We'll use the same precision everywhere, namely a bit width of 6 and 0 integer bits (this will be implemented as``<6,1>`` in hls4ml, due to the missing sign-bit). For now, make sure to set ```use_bias=True``` in ```QConv2DBatchnorm``` to avoid problems during synthesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qkeras import QActivation\n",
    "from qkeras import QDense, QConv2DBatchnorm, QConv2D, quantized_po2, quantized_bits, quantized_relu\n",
    "\n",
    "x = x_in = Input(shape=input_shape, name='input')\n",
    "\n",
    "x = QConv2D(\n",
    "    32, (2, 2), strides=(2,2),\n",
    "    kernel_quantizer=quantized_bits(4,0,1),\n",
    "    bias_quantizer=quantized_bits(4,0,1),\n",
    "    name=\"conv2d_0_m\")(x)\n",
    "x = QActivation(\"quantized_relu(4,0)\", name=\"act0_m\")(x)\n",
    "x = QConv2D(\n",
    "    64, (3, 3), strides=(2,2),\n",
    "    kernel_quantizer=quantized_bits(4,0,1),\n",
    "    bias_quantizer=quantized_bits(4,0,1),\n",
    "    name=\"conv2d_1_m\")(x)\n",
    "x = QActivation(\"quantized_relu(4,0)\", name=\"act1_m\")(x)\n",
    "x = QConv2D(\n",
    "    64, (2, 2), strides=(2,2),\n",
    "    kernel_quantizer=quantized_bits(4,0,1),\n",
    "    bias_quantizer=quantized_bits(4,0,1),\n",
    "    name=\"conv2d_2_m\")(x)\n",
    "x = QActivation(\"quantized_relu(4,0)\", name=\"act2_m\")(x)\n",
    "x = Flatten()(x)\n",
    "x_out = QDense(2, kernel_quantizer=quantized_bits(16,10,1),\n",
    "           bias_quantizer=quantized_bits(16,10,1),\n",
    "           name=\"dense\")(x)\n",
    "\n",
    "qmodel = Model(inputs=[x_in], outputs=[x_out], name='qkeras')\n",
    "\n",
    "qmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the quantized layers\n",
    "from qkeras.autoqkeras.utils import print_qmodel_summary\n",
    "print_qmodel_summary(qmodel)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "You see that a bias quantizer is defined, although we are not using a bias term for the layers. This is set automatically by QKeras. In addition, you'll note that ``alpha='1'``. This sets the weight scale per channel to 1 (no scaling). The default is ``alpha='auto_po2'``, which sets the weight scale per channel to be a power-of-2, such that an actual hardware implementation can be performed by just shifting the result of the convolutional/dense layer to the right or left by checking the sign of the scale and then taking the log2 of the scale.\n",
    "\n",
    "Let's now prune and train this model! If you want, you can also train the unpruned version, ``qmodel`` and see how the performance compares. We will stick to the pruned one here. Again, we do not use a model checkpoint which stores the best weights, in order to ensure the model is trained to the desired sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmodel_pruned = tf.keras.models.clone_model( qmodel , clone_function=pruneFunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4416/4416 [==============================] - 4034s 913ms/step - loss: 41299.1250 - mean_squared_error: 41299.1250 - val_loss: 40784.9414 - val_mean_squared_error: 40784.9414 - lr: 0.0030\n",
      "Epoch 2/2\n",
      "4416/4416 [==============================] - 4030s 913ms/step - loss: 40785.2578 - mean_squared_error: 40785.2539 - val_loss: 40640.9375 - val_mean_squared_error: 40640.9414 - lr: 0.0030\n",
      "\n",
      " It took 134.38976111809413 minutes to train!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train = True\n",
    "\n",
    "n_epochs = 2\n",
    "if train:\n",
    "    LOSS        = tf.keras.losses.MeanSquaredError()\n",
    "    OPTIMIZER   = tf.keras.optimizers.Adam(learning_rate=3E-3, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=True) \n",
    "    qmodel_pruned.compile(loss=LOSS, optimizer=OPTIMIZER, metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "\n",
    "    callbacks = [\n",
    "            tf.keras.callbacks.EarlyStopping(patience=10, verbose=1),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1),\n",
    "            pruning_callbacks.UpdatePruningStep()\n",
    "            ]  \n",
    "\n",
    "    start = time.time()\n",
    "    history = qmodel_pruned.fit(train_data,\n",
    "                          epochs = n_epochs,\n",
    "                          validation_data = val_data,\n",
    "                          callbacks = callbacks, \n",
    "                          verbose=1)     \n",
    "    end = time.time()\n",
    "    print('\\n It took {} minutes to train!\\n'.format( (end - start)/60.))\n",
    "\n",
    "    qmodel_pruned.save('quantized_pruned_vertex_model.h5')\n",
    "\n",
    "else:\n",
    "    from qkeras.utils import _add_supported_quantized_objects\n",
    "    from tensorflow_model_optimization.python.core.sparsity.keras import pruning_wrapper\n",
    "    \n",
    "    co = {}\n",
    "    _add_supported_quantized_objects(co)\n",
    "    co['PruneLowMagnitude'] = pruning_wrapper.PruneLowMagnitude\n",
    "    qmodel_pruned = tf.keras.models.load_model('quantized_pruned_vertex_model.h5', custom_objects=co)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We note that training a model quantization aware, takes around twice as long as when not quantizing during training!\n",
    "The validation accuracy is very similar to that of the floating point model equivalent, despite containing significantly less information \n",
    "\n",
    "## Performance\n",
    "Let's look at some ROC curves to compare the performance. Lets choose a few numbers so it doesn't get confusing. Feel free to change the numbers in ``labels``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 12ms/step\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 77910.8281 - mean_squared_error: 77910.5156\n",
      "10/10 [==============================] - 0s 28ms/step\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 39099.6562 - mean_squared_error: 39099.6562\n",
      "[[  36.789062 ]\n",
      " [ 100.86719  ]\n",
      " [-122.484375 ]\n",
      " [ -98.0625   ]\n",
      " [ 260.83594  ]\n",
      " [-171.89844  ]\n",
      " [ 258.7578   ]\n",
      " [-245.48438  ]\n",
      " [-206.89844  ]\n",
      " [-110.30469  ]\n",
      " [ 230.16406  ]\n",
      " [-118.828125 ]\n",
      " [-138.29688  ]\n",
      " [ 325.       ]\n",
      " [-250.75     ]\n",
      " [ 103.57031  ]\n",
      " [   1.515625 ]\n",
      " [  37.875    ]\n",
      " [ 324.96875  ]\n",
      " [ 193.125    ]\n",
      " [ -20.617188 ]\n",
      " [ -52.757812 ]\n",
      " [ -79.921875 ]\n",
      " [ 373.28125  ]\n",
      " [ -37.546875 ]\n",
      " [-300.84375  ]\n",
      " [ -19.773438 ]\n",
      " [ 207.9375   ]\n",
      " [ 100.328125 ]\n",
      " [-138.13281  ]\n",
      " [-107.109375 ]\n",
      " [ 286.4375   ]\n",
      " [-123.296875 ]\n",
      " [-122.71875  ]\n",
      " [-271.0703   ]\n",
      " [-173.33594  ]\n",
      " [  29.007812 ]\n",
      " [ 263.65625  ]\n",
      " [ 175.98438  ]\n",
      " [ 177.85938  ]\n",
      " [-307.33594  ]\n",
      " [-339.6172   ]\n",
      " [ 340.14062  ]\n",
      " [-285.83594  ]\n",
      " [ -67.46094  ]\n",
      " [-104.265625 ]\n",
      " [ 218.97656  ]\n",
      " [ -53.453125 ]\n",
      " [ 182.96875  ]\n",
      " [ 352.0625   ]\n",
      " [ 109.99219  ]\n",
      " [ 284.5625   ]\n",
      " [-146.83594  ]\n",
      " [-107.359375 ]\n",
      " [ 204.36719  ]\n",
      " [ 379.625    ]\n",
      " [ 273.4375   ]\n",
      " [-118.515625 ]\n",
      " [ -51.304688 ]\n",
      " [-368.90625  ]\n",
      " [-147.36719  ]\n",
      " [ 210.82812  ]\n",
      " [ 280.84375  ]\n",
      " [ 130.53906  ]\n",
      " [-216.82812  ]\n",
      " [-219.02344  ]\n",
      " [-122.16406  ]\n",
      " [ 173.04688  ]\n",
      " [-107.42969  ]\n",
      " [ -80.86719  ]\n",
      " [-122.375    ]\n",
      " [ 139.60938  ]\n",
      " [ 269.79688  ]\n",
      " [-152.       ]\n",
      " [ 351.96094  ]\n",
      " [-304.03125  ]\n",
      " [-170.76562  ]\n",
      " [ -32.453125 ]\n",
      " [ 271.66406  ]\n",
      " [ 236.70312  ]\n",
      " [ 280.75     ]\n",
      " [-130.96094  ]\n",
      " [ 253.00781  ]\n",
      " [ 244.85156  ]\n",
      " [-180.89844  ]\n",
      " [ -43.476562 ]\n",
      " [  71.125    ]\n",
      " [ 386.10938  ]\n",
      " [-305.27344  ]\n",
      " [ -42.789062 ]\n",
      " [  99.875    ]\n",
      " [-387.5      ]\n",
      " [ -40.484375 ]\n",
      " [ -41.101562 ]\n",
      " [-295.67188  ]\n",
      " [ -34.875    ]\n",
      " [ -24.0625   ]\n",
      " [ -55.117188 ]\n",
      " [ 329.84375  ]\n",
      " [ 127.10156  ]\n",
      " [  78.99219  ]\n",
      " [ -63.0625   ]\n",
      " [-382.14062  ]\n",
      " [ -31.445312 ]\n",
      " [ -87.734375 ]\n",
      " [-242.46875  ]\n",
      " [ 232.38281  ]\n",
      " [ -31.125    ]\n",
      " [-173.55469  ]\n",
      " [ 332.25     ]\n",
      " [ 181.625    ]\n",
      " [-146.72656  ]\n",
      " [ 339.54688  ]\n",
      " [  -0.609375 ]\n",
      " [ -18.359375 ]\n",
      " [-275.64062  ]\n",
      " [  26.023438 ]\n",
      " [-215.97656  ]\n",
      " [ -62.523438 ]\n",
      " [-104.140625 ]\n",
      " [ 205.79688  ]\n",
      " [  72.75     ]\n",
      " [ 120.234375 ]\n",
      " [ 205.89062  ]\n",
      " [ 194.85156  ]\n",
      " [-195.20312  ]\n",
      " [ 103.859375 ]\n",
      " [  74.78906  ]\n",
      " [  29.445312 ]\n",
      " [  13.2578125]\n",
      " [ -10.1015625]\n",
      " [-233.6875   ]\n",
      " [ 225.44531  ]\n",
      " [ 130.38281  ]\n",
      " [ 150.65625  ]\n",
      " [ 298.9453   ]\n",
      " [  17.59375  ]\n",
      " [ 187.46875  ]\n",
      " [ -10.046875 ]\n",
      " [-185.03125  ]\n",
      " [ 156.16406  ]\n",
      " [  71.46875  ]\n",
      " [-190.16406  ]\n",
      " [ 383.9375   ]\n",
      " [-247.32031  ]\n",
      " [-213.8125   ]\n",
      " [  -9.34375  ]\n",
      " [  -7.390625 ]\n",
      " [-353.5      ]\n",
      " [  24.25     ]\n",
      " [-228.96875  ]\n",
      " [  32.828125 ]\n",
      " [ 272.0703   ]\n",
      " [  78.40625  ]\n",
      " [-241.11719  ]\n",
      " [-166.85156  ]\n",
      " [  56.289062 ]\n",
      " [ 104.80469  ]\n",
      " [ 290.39844  ]\n",
      " [ -35.523438 ]\n",
      " [ 143.95312  ]\n",
      " [-145.47656  ]\n",
      " [-150.6875   ]\n",
      " [-104.33594  ]\n",
      " [ 214.61719  ]\n",
      " [ -28.125    ]\n",
      " [ 176.71875  ]\n",
      " [-117.5625   ]\n",
      " [-301.9922   ]\n",
      " [ 269.75     ]\n",
      " [  76.86719  ]\n",
      " [ -27.375    ]\n",
      " [  30.53125  ]\n",
      " [  99.0625   ]\n",
      " [ 253.86719  ]\n",
      " [ -46.789062 ]\n",
      " [ 266.03906  ]\n",
      " [ -29.625    ]\n",
      " [ 229.29688  ]\n",
      " [ 182.39844  ]\n",
      " [  94.65625  ]\n",
      " [-178.08594  ]\n",
      " [ -42.257812 ]\n",
      " [ 116.35156  ]\n",
      " [   9.953125 ]\n",
      " [ 155.14844  ]\n",
      " [ -16.179688 ]\n",
      " [ 181.59375  ]\n",
      " [ 177.92188  ]\n",
      " [  93.27344  ]\n",
      " [ 102.44531  ]\n",
      " [ -72.421875 ]\n",
      " [-327.65625  ]\n",
      " [  10.9609375]\n",
      " [ 239.02344  ]\n",
      " [-201.54688  ]\n",
      " [  49.195312 ]\n",
      " [  24.4375   ]\n",
      " [-260.1797   ]\n",
      " [ 128.75781  ]\n",
      " [ -43.804688 ]\n",
      " [-286.8047   ]\n",
      " [ 228.4375   ]\n",
      " [-193.10156  ]\n",
      " [-222.47656  ]\n",
      " [ 189.84375  ]\n",
      " [ 125.28906  ]\n",
      " [ -18.53125  ]\n",
      " [  39.398438 ]\n",
      " [-135.58594  ]\n",
      " [ 311.85156  ]\n",
      " [-188.21094  ]\n",
      " [-361.875    ]\n",
      " [-234.85938  ]\n",
      " [ 182.25781  ]\n",
      " [ 391.7422   ]\n",
      " [ 272.5625   ]\n",
      " [  64.       ]\n",
      " [ -75.10156  ]\n",
      " [ -93.02344  ]\n",
      " [-243.29688  ]\n",
      " [ -82.125    ]\n",
      " [  -5.796875 ]\n",
      " [-292.08594  ]\n",
      " [  45.554688 ]\n",
      " [-128.63281  ]\n",
      " [-291.4453   ]\n",
      " [ 269.8125   ]\n",
      " [ 213.33594  ]\n",
      " [-305.07812  ]\n",
      " [  17.671875 ]\n",
      " [-191.17188  ]\n",
      " [  88.546875 ]\n",
      " [ 308.46094  ]\n",
      " [  84.875    ]\n",
      " [-154.89844  ]\n",
      " [-281.4922   ]\n",
      " [ 353.65625  ]\n",
      " [ 197.45312  ]\n",
      " [ 171.875    ]\n",
      " [ 164.45312  ]\n",
      " [ -54.523438 ]\n",
      " [  38.59375  ]\n",
      " [-282.92188  ]\n",
      " [-264.67188  ]\n",
      " [ 241.05469  ]\n",
      " [-203.39844  ]\n",
      " [ -32.015625 ]\n",
      " [  -9.84375  ]\n",
      " [ 259.3672   ]\n",
      " [ 221.85938  ]\n",
      " [  15.953125 ]\n",
      " [  22.132812 ]\n",
      " [ 141.32812  ]\n",
      " [-275.5703   ]\n",
      " [ 307.28125  ]\n",
      " [ -38.703125 ]\n",
      " [ 120.41406  ]\n",
      " [-281.8047   ]\n",
      " [-305.03125  ]\n",
      " [-263.25     ]\n",
      " [-103.9375   ]\n",
      " [ 109.90625  ]\n",
      " [-376.10938  ]\n",
      " [ 117.39844  ]\n",
      " [-423.29688  ]\n",
      " [ 179.74219  ]\n",
      " [  -4.4140625]\n",
      " [ 123.91406  ]\n",
      " [-216.39844  ]\n",
      " [-120.984375 ]\n",
      " [   1.25     ]\n",
      " [ 306.9297   ]\n",
      " [-314.375    ]\n",
      " [ -93.66406  ]\n",
      " [ -56.453125 ]\n",
      " [ -29.78125  ]\n",
      " [ 201.95312  ]\n",
      " [  38.375    ]\n",
      " [-105.984375 ]\n",
      " [ 240.42969  ]\n",
      " [ 163.70312  ]\n",
      " [ -46.726562 ]\n",
      " [-141.54688  ]\n",
      " [  63.976562 ]\n",
      " [  62.265625 ]\n",
      " [ 144.55469  ]\n",
      " [ 211.09375  ]\n",
      " [-283.40625  ]\n",
      " [ 315.6172   ]\n",
      " [-102.859375 ]\n",
      " [ -74.74219  ]\n",
      " [ 271.3203   ]\n",
      " [ -77.64844  ]\n",
      " [ 185.86719  ]\n",
      " [  39.648438 ]\n",
      " [ 257.71094  ]\n",
      " [-174.49219  ]\n",
      " [  41.25     ]\n",
      " [ 113.9375   ]] tf.Tensor(\n",
      "[[ -94.733864   154.83783  ]\n",
      " [-248.17625    402.4189   ]\n",
      " [-276.01068     19.052181 ]\n",
      " [ -42.36755   -143.8818   ]\n",
      " [ 455.20682     65.27731  ]\n",
      " [-376.46353     28.976192 ]\n",
      " [ 555.5444     -14.726693 ]\n",
      " [-188.16707   -353.64056  ]\n",
      " [-293.5057    -139.10916  ]\n",
      " [-255.07295     24.698122 ]\n",
      " [ 274.04227    199.3483   ]\n",
      " [ -86.77234   -158.87828  ]\n",
      " [  12.854623  -300.83185  ]\n",
      " [ 516.78784    110.014015 ]\n",
      " [-173.75134   -351.436    ]\n",
      " [ 365.71292     30.677343 ]\n",
      " [-246.17441    291.47476  ]\n",
      " [-131.90692    218.07921  ]\n",
      " [ 404.96268    393.10947  ]\n",
      " [ 153.35251    272.81985  ]\n",
      " [  68.32282   -115.17167  ]\n",
      " [ 209.10454   -452.3426   ]\n",
      " [-400.69635    251.14647  ]\n",
      " [ 206.6794     508.03354  ]\n",
      " [-174.7004      95.00323  ]\n",
      " [-332.89484   -406.65955  ]\n",
      " [-372.4589     362.23236  ]\n",
      " [ -13.699848   440.09872  ]\n",
      " [-272.24487    472.2461   ]\n",
      " [ -91.46979   -170.64238  ]\n",
      " [-283.92615     63.541252 ]\n",
      " [  65.18532    510.99545  ]\n",
      " [-347.50815     92.76481  ]\n",
      " [ 221.86757   -438.6481   ]\n",
      " [-196.49788   -342.58224  ]\n",
      " [ -66.87967   -266.99158  ]\n",
      " [-298.27814    331.36926  ]\n",
      " [ -93.10166    543.4161   ]\n",
      " [ 391.0508     -14.284768 ]\n",
      " [ 531.56635   -195.32417  ]\n",
      " [-187.48973   -421.50504  ]\n",
      " [-506.1521    -186.77742  ]\n",
      " [ 347.3597     385.01398  ]\n",
      " [-161.27934   -402.2837   ]\n",
      " [ 138.49666   -265.0847   ]\n",
      " [  49.498257  -264.62985  ]\n",
      " [ 212.7778     230.73651  ]\n",
      " [  68.389786  -168.26334  ]\n",
      " [  92.31803    293.38058  ]\n",
      " [ 430.8778     297.56558  ]\n",
      " [  85.98564    162.70569  ]\n",
      " [ 172.39967    389.72147  ]\n",
      " [-134.33437   -162.96088  ]\n",
      " [  55.739864  -281.42374  ]\n",
      " [ -10.664309   447.01328  ]\n",
      " [ 515.3662     207.31232  ]\n",
      " [ 367.48044    187.96622  ]\n",
      " [-136.02853   -153.11737  ]\n",
      " [ -30.441046   -51.489494 ]\n",
      " [-518.0069    -208.50133  ]\n",
      " [ -40.288155  -245.4575   ]\n",
      " [ 323.60797    123.77361  ]\n",
      " [ 419.28723    125.13153  ]\n",
      " [  56.448227   201.79135  ]\n",
      " [  55.34778   -518.68634  ]\n",
      " [-214.10475   -247.31715  ]\n",
      " [ -20.823727  -194.5705   ]\n",
      " [ 349.5617     -14.804919 ]\n",
      " [-193.03305    -16.563824 ]\n",
      " [ 251.97131   -404.61664  ]\n",
      " [ 157.23743   -427.63367  ]\n",
      " [ 143.62737    194.64676  ]\n",
      " [ 508.7353      17.702269 ]\n",
      " [-354.13742     39.159115 ]\n",
      " [ 392.2957     289.7233   ]\n",
      " [-405.04553   -202.05092  ]\n",
      " [-292.78656    -30.306372 ]\n",
      " [-325.70096    227.28705  ]\n",
      " [ 285.7589     268.96323  ]\n",
      " [ 460.56534     11.021701 ]\n",
      " [  49.381077   497.57104  ]\n",
      " [-389.93512     58.33563  ]\n",
      " [ 388.8447      87.08337  ]\n",
      " [ 391.32562    101.679985 ]\n",
      " [-444.0685      68.70448  ]\n",
      " [-258.95276    170.8037   ]\n",
      " [-282.58328    438.75037  ]\n",
      " [ 412.63986    366.00305  ]\n",
      " [-174.90088   -418.12442  ]\n",
      " [ -89.512115    -1.6612415]\n",
      " [ 352.62247   -144.94302  ]\n",
      " [-474.09518   -307.41266  ]\n",
      " [-294.72906    214.27635  ]\n",
      " [ 320.54932   -347.82513  ]\n",
      " [  10.460076  -555.90314  ]\n",
      " [ 160.93912   -253.01808  ]\n",
      " [-188.69156    140.19478  ]\n",
      " [  87.1562    -167.77287  ]\n",
      " [ 111.494026   523.8619   ]\n",
      " [ -55.895905   318.0251   ]\n",
      " [-216.33733    384.3813   ]\n",
      " [ -30.792562  -106.465294 ]\n",
      " [-222.52628   -448.73755  ]\n",
      " [ 306.46295   -365.4215   ]\n",
      " [-435.63416    248.01212  ]\n",
      " [ -46.807396  -424.0976   ]\n",
      " [ 437.98654     64.479126 ]\n",
      " [-340.03195    288.3507   ]\n",
      " [  57.56492   -447.51184  ]\n",
      " [ 431.511      249.46182  ]\n",
      " [ 262.07962    105.49695  ]\n",
      " [ -42.322166  -251.1028   ]\n",
      " [ 239.23671    436.2437   ]\n",
      " [-290.34882    180.33444  ]\n",
      " [ -28.714073   -22.728146 ]\n",
      " [-421.82895   -106.69975  ]\n",
      " [-263.0079     317.67184  ]\n",
      " [  22.653757  -464.1067   ]\n",
      " [  57.986313  -161.85742  ]\n",
      " [-442.292      241.08572  ]\n",
      " [ 352.3645      60.168354 ]\n",
      " [ 421.988     -240.20845  ]\n",
      " [ 389.89566   -128.25905  ]\n",
      " [  75.83888    346.76334  ]\n",
      " [ 265.2091     136.98405  ]\n",
      " [ -47.827835  -555.211    ]\n",
      " [ 254.22919    -39.24102  ]\n",
      " [-286.8429     432.70114  ]\n",
      " [-222.33914    301.32077  ]\n",
      " [ -58.866825   116.42463  ]\n",
      " [ -60.9865      48.850216 ]\n",
      " [-268.0937    -233.10931  ]\n",
      " [ 394.28976     43.501774 ]\n",
      " [ 419.5806    -152.8148   ]\n",
      " [ -41.451546   322.48413  ]\n",
      " [ 220.98358    398.50513  ]\n",
      " [ -92.77465    134.03064  ]\n",
      " [ 498.92294   -114.736145 ]\n",
      " [-370.54172    312.79657  ]\n",
      " [  60.207073  -453.49173  ]\n",
      " [ 391.11685    -83.43107  ]\n",
      " [ 287.5449     -88.79392  ]\n",
      " [  83.229805  -481.54605  ]\n",
      " [ 416.20367    375.0282   ]\n",
      " [-476.5998     -26.575346 ]\n",
      " [-320.938      -96.26683  ]\n",
      " [ -20.150747   -20.712076 ]\n",
      " [-284.4855     292.13486  ]\n",
      " [-308.2453    -374.1955   ]\n",
      " [ 317.19315   -268.5177   ]\n",
      " [ -32.946068  -402.07867  ]\n",
      " [-161.10565    216.38162  ]\n",
      " [ 269.60275    346.5872   ]\n",
      " [ 136.94128     62.98978  ]\n",
      " [  34.295795  -536.5446   ]\n",
      " [-201.57236   -175.0312   ]\n",
      " [  74.54329     60.216183 ]\n",
      " [ 141.33246     69.62615  ]\n",
      " [ 202.75066    410.37708  ]\n",
      " [-262.84415    194.67867  ]\n",
      " [ 434.82016   -124.40274  ]\n",
      " [-355.45227     53.335438 ]\n",
      " [ 207.49672   -495.63147  ]\n",
      " [-313.28366     96.70212  ]\n",
      " [ 433.5354      18.941128 ]\n",
      " [-326.93063    177.02289  ]\n",
      " [ 342.84915     17.203812 ]\n",
      " [  38.34681   -277.434    ]\n",
      " [-327.7424    -306.38556  ]\n",
      " [ 399.30054    103.72843  ]\n",
      " [-247.97969    411.09937  ]\n",
      " [-242.44518    197.14084  ]\n",
      " [ -90.39602     57.205994 ]\n",
      " [-113.88828    366.5109   ]\n",
      " [ 344.6785     172.36838  ]\n",
      " [  21.437288  -115.5094   ]\n",
      " [ 278.93298    244.59842  ]\n",
      " [-291.37445    255.90356  ]\n",
      " [ 434.34003     63.48452  ]\n",
      " [ 285.26886    217.96858  ]\n",
      " [ 446.9837    -221.53508  ]\n",
      " [ 179.07484   -510.02603  ]\n",
      " [ 326.90433   -447.0356   ]\n",
      " [ 220.44064     24.991863 ]\n",
      " [-312.065      368.893    ]\n",
      " [   7.6385713  274.36142  ]\n",
      " [-196.67078    162.88268  ]\n",
      " [ 234.02484    208.49992  ]\n",
      " [ 455.02206    -94.101974 ]\n",
      " [ 412.33875   -188.5291   ]\n",
      " [  35.689365   169.14189  ]\n",
      " [ 304.48837   -227.98796  ]\n",
      " [-192.53993   -531.7725   ]\n",
      " [ 402.63358   -383.44125  ]\n",
      " [ -17.128742   497.49045  ]\n",
      " [-414.0671      10.925389 ]\n",
      " [ 218.91867    -47.050735 ]\n",
      " [-371.36893    408.1907   ]\n",
      " [-216.3336    -340.7113   ]\n",
      " [ 487.18784   -220.432    ]\n",
      " [  -2.18372   -105.19097  ]\n",
      " [-551.7391     -73.35202  ]\n",
      " [ 312.8944     219.34372  ]\n",
      " [  88.7574    -461.5752   ]\n",
      " [-114.830414  -332.54892  ]\n",
      " [ 415.87143    -15.525423 ]\n",
      " [ -10.750234   256.1489   ]\n",
      " [ 319.47574   -335.65182  ]\n",
      " [-133.10759    221.295    ]\n",
      " [ -22.597012  -250.92307  ]\n",
      " [ 368.91544    231.52382  ]\n",
      " [-247.07123   -141.43784  ]\n",
      " [-277.81686   -468.11078  ]\n",
      " [ -29.857185  -429.4505   ]\n",
      " [ 220.80669    149.46236  ]\n",
      " [ 314.1801     363.15698  ]\n",
      " [ 229.8649     370.37396  ]\n",
      " [ -39.46399    185.8923   ]\n",
      " [-183.6933      40.847904 ]\n",
      " [-463.46967    127.2632   ]\n",
      " [ -90.95677   -355.0479   ]\n",
      " [ -22.5262    -116.8886   ]\n",
      " [  15.094225    -7.075676 ]\n",
      " [-313.5952    -252.63562  ]\n",
      " [ 289.4492    -176.20049  ]\n",
      " [-136.28394    -72.008446 ]\n",
      " [-418.61322   -178.4434   ]\n",
      " [ 382.51627    186.9498   ]\n",
      " [ 263.48077    261.3916   ]\n",
      " [-499.99402   -112.889786 ]\n",
      " [-140.04475    165.40591  ]\n",
      " [-282.0604     -90.31881  ]\n",
      " [ 291.05078    -95.1827   ]\n",
      " [ 262.8897     397.21124  ]\n",
      " [ 277.19077    -72.72     ]\n",
      " [-464.9597     176.81628  ]\n",
      " [-280.37692   -338.96384  ]\n",
      " [ 506.53195    195.0644   ]\n",
      " [ 394.07794     27.948849 ]\n",
      " [ 233.9536     224.17686  ]\n",
      " [ 236.53674    103.608444 ]\n",
      " [  19.486921  -166.0987   ]\n",
      " [ 404.52478   -356.91467  ]\n",
      " [-365.6829    -216.08264  ]\n",
      " [-125.94582   -412.41547  ]\n",
      " [ 526.96265    -31.441206 ]\n",
      " [ -41.178333  -369.9848   ]\n",
      " [  25.088158   -79.34181  ]\n",
      " [-161.26364    160.5281   ]\n",
      " [ 253.67447    303.94363  ]\n",
      " [ -83.08873    497.4269   ]\n",
      " [ 107.09971    -69.93613  ]\n",
      " [ 174.25415   -126.423416 ]\n",
      " [ 372.6396    -213.18102  ]\n",
      " [-549.61835      3.181044 ]\n",
      " [ 205.25339    415.02448  ]\n",
      " [-362.742      311.27386  ]\n",
      " [  73.29536    193.7371   ]\n",
      " [-345.59958   -222.63127  ]\n",
      " [-239.74088   -497.21005  ]\n",
      " [-550.4287      27.948488 ]\n",
      " [-142.57556    -70.07791  ]\n",
      " [  -1.6776547  233.77687  ]\n",
      " [-467.5683    -316.77814  ]\n",
      " [  29.2535     204.53009  ]\n",
      " [-357.66296   -369.9383   ]\n",
      " [ 364.97208    -10.045477 ]\n",
      " [-364.44977    376.83444  ]\n",
      " [ 163.74214     93.0381   ]\n",
      " [-367.9239     -98.54777  ]\n",
      " [-330.91528     82.45896  ]\n",
      " [-179.0976     195.59709  ]\n",
      " [  68.21718    493.94202  ]\n",
      " [-100.837265  -485.2929   ]\n",
      " [ 151.07918   -348.2012   ]\n",
      " [  84.9721    -187.44751  ]\n",
      " [-355.66602    296.10373  ]\n",
      " [  -4.096187   405.59662  ]\n",
      " [  63.071255    22.653648 ]\n",
      " [-362.86813    127.5847   ]\n",
      " [  87.893074   392.47964  ]\n",
      " [ 459.41684   -115.155876 ]\n",
      " [ -20.199816   -54.812504 ]\n",
      " [-515.92096    210.51297  ]\n",
      " [ -94.03063    241.39127  ]\n",
      " [ 452.0761    -328.0728   ]\n",
      " [ 183.75642    104.271416 ]\n",
      " [ 317.71875    118.41702  ]\n",
      " [ -47.38297   -486.6965   ]\n",
      " [  96.94958    518.2209   ]\n",
      " [ 244.6823    -463.7771   ]\n",
      " [-361.3467     132.21004  ]\n",
      " [ 270.4389     324.94614  ]\n",
      " [-306.3871     164.07814  ]\n",
      " [   6.6371913  372.21738  ]\n",
      " [ 155.85925    -66.75071  ]\n",
      " [ 331.1411     227.87212  ]\n",
      " [ -49.58161   -302.20258  ]\n",
      " [  38.87613     43.927788 ]\n",
      " [  74.98804    134.5932   ]], shape=(300, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "predict_baseline    = model_pruned.predict(X_test)\n",
    "test_score_baseline = model_pruned.evaluate(X_test, Y_test)\n",
    "\n",
    "predict_qkeras    = qmodel_pruned.predict(X_test)\n",
    "test_score_qkeras = qmodel_pruned.evaluate(X_test, Y_test)\n",
    "print(predict_qkeras, Y_test)\n",
    "\n",
    "print('Keras accuracy = {} , QKeras 6-bit accuracy = {}'.format(test_score_baseline[1],test_score_qkeras[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "labels=['%i'%nr for nr in range (0,n_classes)] # If you want to look at all the labels\n",
    "# labels = ['0','1','9'] # Look at only a few labels, here for digits 0, 1 and 9\n",
    "print('Plotting ROC for labels {}'.format(labels))\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df_q = pd.DataFrame()\n",
    "fpr  = {}\n",
    "tpr  = {}\n",
    "auc1 = {}\n",
    "fpr_q  = {}\n",
    "tpr_q  = {}\n",
    "auc1_q = {}\n",
    "%matplotlib inline\n",
    "colors  = ['#67001f','#b2182b','#d6604d','#f4a582','#fddbc7','#d1e5f0','#92c5de','#4393c3','#2166ac','#053061']\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "for i, label in enumerate(labels):\n",
    "    df[label] = Y_test[:,int(label)]\n",
    "    df[label + '_pred'] = predict_baseline[:,int(label)]\n",
    "    fpr[label], tpr[label], threshold = metrics.roc_curve(df[label],df[label+'_pred'])\n",
    "    auc1[label] = metrics.auc(fpr[label], tpr[label])\n",
    "    \n",
    "    df_q[label] = Y_test[:,int(label)]\n",
    "    df_q[label + '_pred'] = predict_qkeras[:,int(label)]\n",
    "    fpr_q[label], tpr_q[label], threshold_q = metrics.roc_curve(df_q[label],df_q[label+'_pred'])\n",
    "    auc1_q[label] = metrics.auc(fpr_q[label], tpr_q[label])\n",
    "    \n",
    "    plt.plot(fpr[label],tpr[label] )#   ,label=r'{}, AUC Keras = {:.1f}% AUC QKeras = {:.1f}%)'.format(label,auc1[label]*100,auc1_q[label]*100), linewidth=1.5,c=colors[i],linestyle='solid')\n",
    "    #plt.plot(fpr_q[label],tpr_q[label], linewidth=1.5,c=colors[i],linestyle='dotted')\n",
    "\n",
    "plt.semilogx()\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.xlim(0.01,1.)\n",
    "plt.ylim(0.5,1.1)\n",
    "plt.legend(loc='lower right')\n",
    "#plt.figtext(0.2, 0.83,r'Accuracy Keras = {:.1f}% QKeras 8-bit = {:.1f}%'.format(test_score_baseline[1]*100,test_score_qkeras[1]*100), wrap=True, horizontalalignment='left',verticalalignment='center')\n",
    "from matplotlib.lines import Line2D\n",
    "lines = [Line2D([0], [0], ls='-'),\n",
    "         Line2D([0], [0], ls='--')]\n",
    "from matplotlib.legend import Legend\n",
    "#leg = Legend(ax, lines, labels=['Keras', 'QKeras'],\n",
    "#            loc='lower right', frameon=False)\n",
    "ax.add_artist(leg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The difference in AUC between the fp32 Keras model and the 8-bit QKeras model, is small, as we have seen for the previous examples. You can find a bonus exercise below, **Bonus: Automatic quantization**, where we'll use AutoQKeras to find the best heterogeneously quantized model, given a set of resource and accuracy constriants.\n",
    "### Check sparsity\n",
    "Let's also check the per-layer sparsity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doWeights(model):\n",
    "\n",
    "    allWeightsByLayer = {}\n",
    "    for layer in model.layers:\n",
    "        if (layer._name).find(\"batch\")!=-1 or len(layer.get_weights())<1:\n",
    "            continue \n",
    "        weights=layer.weights[0].numpy().flatten()  \n",
    "        allWeightsByLayer[layer._name] = weights\n",
    "        print('Layer {}: % of zeros = {}'.format(layer._name,np.sum(weights==0)/np.size(weights)))\n",
    "\n",
    "    labelsW = []\n",
    "    histosW = []\n",
    "\n",
    "    for key in reversed(sorted(allWeightsByLayer.keys())):\n",
    "        labelsW.append(key)\n",
    "        histosW.append(allWeightsByLayer[key])\n",
    "\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    bins = np.linspace(-1.5, 1.5, 50)\n",
    "    plt.hist(histosW,bins,histtype='stepfilled',stacked=True,label=labelsW, edgecolor='black')\n",
    "    plt.legend(frameon=False,loc='upper left')\n",
    "    plt.ylabel('Number of Weights')\n",
    "    plt.xlabel('Weights')\n",
    "    plt.figtext(0.2, 0.38,model._name, wrap=True, horizontalalignment='left',verticalalignment='center')\n",
    "    \n",
    "doWeights(model_pruned) \n",
    "doWeights(qmodel_pruned) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We see that 50% of the weights per layer are set to zero, as expected.\n",
    "Now, let's synthesize the floating point Keras model and the QKeras quantized model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## CNNs in hls4ml\n",
    "\n",
    "In this part, we will take the two models we trained above (the floating-point 32 Keras model and the 6-bit QKeras model), and synthesize them with hls4ml. Although your models are probably already in memory, let's load them from scratch. We need to pass the appropriate custom QKeras/pruning layers when loading, and remove the pruning parameters that were saved together with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_model_optimization.sparsity.keras import strip_pruning\n",
    "from tensorflow_model_optimization.python.core.sparsity.keras import pruning_wrapper\n",
    "\n",
    "from qkeras.utils import _add_supported_quantized_objects\n",
    "\n",
    "co = {}\n",
    "_add_supported_quantized_objects(co)\n",
    "co['PruneLowMagnitude'] = pruning_wrapper.PruneLowMagnitude\n",
    "\n",
    "model = tf.keras.models.load_model('pruned_cnn_model.h5',custom_objects=co)\n",
    "model  = strip_pruning(model)\n",
    "\n",
    "qmodel = tf.keras.models.load_model('quantized_pruned_cnn_model.h5',custom_objects=co)\n",
    "qmodel  = strip_pruning(qmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to define the hls4ml and Vivado configurations. Two things will change with respect to what was done in the previous exercises. First, we will use ``IOType= 'io_stream'`` in the Vivado configuration.\n",
    "\n",
    "---\n",
    "****You must use ``IOType= 'io_stream'`` if attempting to synthesize a convolutional neural network.****\n",
    "\n",
    "---\n",
    "The CNN implementation in hls4ml is based on streams, which are synthesized in hardware as first in, first out (FIFO) buffers. Shift registers are used to keep track of the last  ``<kernel height - 1>`` rows of input pixels, and maintains a shifting snapshot of the convolution kernel.\n",
    "\n",
    "This is illustrated  in the gif below. Here, the input image is at the top-left and the output image at the bottom left. The top right image shows the internal state of the shift registers and convolutional kernel. The red square indicates the current pixels contained within the convolutional kernel.\n",
    "\n",
    "![alt text](images/conv2d_animation.gif \"The implementation of convolutional layers in hls4ml.\")\n",
    "\n",
    "Lastly, we will use ``['Strategy'] = 'Latency'`` for all the layers in the hls4ml configuration. If one layer would have >4096 elements, we sould set ``['Strategy'] = 'Resource'`` for that layer, or increase the reuse factor by hand. You can find examples of how to do this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hls4ml\n",
    "import plotting\n",
    "\n",
    "hls4ml.model.optimizer.OutputRoundingSaturationMode.layers = ['Activation']\n",
    "hls4ml.model.optimizer.OutputRoundingSaturationMode.rounding_mode = 'AP_RND'\n",
    "hls4ml.model.optimizer.OutputRoundingSaturationMode.saturation_mode = 'AP_SAT'\n",
    "\n",
    "#First, the baseline model\n",
    "hls_config = hls4ml.utils.config_from_keras_model(model, granularity='name')\n",
    "\n",
    "# Set the precision and reuse factor for the full model\n",
    "hls_config['Model']['Precision'] = 'ap_fixed<16,6>'\n",
    "hls_config['Model']['ReuseFactor'] = 1\n",
    "\n",
    "# Create an entry for each layer, here you can for instance change the strategy for a layer to 'resource' \n",
    "# or increase the reuse factor individually for large layers.\n",
    "# In this case, we designed the model to be small enough for a fully parallel implementation \n",
    "# so we use the latency strategy and reuse factor of 1 for all layers.\n",
    "for Layer in hls_config['LayerName'].keys():\n",
    "    hls_config['LayerName'][Layer]['Strategy'] = 'Latency'\n",
    "    hls_config['LayerName'][Layer]['ReuseFactor'] = 1\n",
    "#If you want best numerical performance for high-accuray models, while the default latency strategy is faster but numerically more unstable\n",
    "hls_config['LayerName']['output_softmax']['Strategy'] = 'Stable'\n",
    "plotting.print_dict(hls_config)\n",
    "\n",
    "cfg = hls4ml.converters.create_config(backend='Vivado')\n",
    "cfg['IOType']     = 'io_stream' # Must set this if using CNNs!\n",
    "cfg['HLSConfig']  = hls_config\n",
    "cfg['KerasModel'] = model\n",
    "cfg['OutputDir']  = 'pruned_cnn/'\n",
    "cfg['XilinxPart'] = 'xcu250-figd2104-2L-e'\n",
    "  \n",
    "hls_model = hls4ml.converters.keras_to_hls(cfg)\n",
    "hls_model.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Let's get a nice overview over the various shapes and precisions used for each layer through ``hls4ml.utils.plot_model``, as well as look at the weight profile using ``hls4ml.model.profiling.numerical``. The weight profiling returns two plots: Before (top) and after (bottom) various optimizations applied to the HLS model before the final translation to HLS, for instance the fusing of Dense and BatchNormalization layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hls4ml.utils.plot_model(hls_model, show_shapes=True, show_precision=True, to_file=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hls4ml.model.profiling.numerical(model=model, hls_model=hls_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The colored boxes are the distribution of the weights of the model, and the gray band illustrates the numerical range covered by the chosen fixed point precision. As we configured, this model uses a precision of ``ap_fixed<16,6>`` for all layers of the model. Let's now build our QKeras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then the QKeras model\n",
    "hls4ml.model.optimizer.OutputRoundingSaturationMode.layers = ['Activation']\n",
    "hls4ml.model.optimizer.OutputRoundingSaturationMode.rounding_mode = 'AP_RND'\n",
    "hls4ml.model.optimizer.OutputRoundingSaturationMode.saturation_mode = 'AP_SAT'\n",
    "\n",
    "hls_config_q = hls4ml.utils.config_from_keras_model(qmodel, granularity='name')\n",
    "hls_config_q['Model']['ReuseFactor'] = 1\n",
    "hls_config['Model']['Precision'] = 'ap_fixed<16,6>'\n",
    "hls_config_q['LayerName']['output_softmax']['Strategy'] = 'Stable'\n",
    "plotting.print_dict(hls_config_q)\n",
    "  \n",
    "cfg_q = hls4ml.converters.create_config(backend='Vivado')\n",
    "cfg_q['IOType']     = 'io_stream' # Must set this if using CNNs!\n",
    "cfg_q['HLSConfig']  = hls_config_q\n",
    "cfg_q['KerasModel'] = qmodel\n",
    "cfg_q['OutputDir']  = 'quantized_pruned_cnn/'\n",
    "cfg_q['XilinxPart'] = 'xcu250-figd2104-2L-e'\n",
    "  \n",
    "hls_model_q = hls4ml.converters.keras_to_hls(cfg_q)\n",
    "hls_model_q.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Let's plot the model and profile the weights her too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hls4ml.model.profiling.numerical(model=qmodel, hls_model=hls_model_q)\n",
    "hls4ml.utils.plot_model(hls_model_q, show_shapes=True, show_precision=True, to_file=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "For the 6-bit QKeras model, we see that different precisions are used for different layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Accuracy with bit-accurate emulation \n",
    "Let's check that the hls4ml accuracy matches the original. This usually takes some time, so let's do it over a reduced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_reduced = X_test[:100]\n",
    "Y_test_reduced = Y_test[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict        = model.predict(X_test_reduced)\n",
    "y_predict_hls4ml = hls_model.predict(np.ascontiguousarray(X_test_reduced))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_q        = qmodel.predict(X_test_reduced)\n",
    "y_predict_hls4ml_q = hls_model_q.predict(np.ascontiguousarray(X_test_reduced))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotting\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def plotROC(Y, y_pred, y_pred_hls4ml, label=\"Model\"):\n",
    "    \n",
    "    accuracy_keras  = float(accuracy_score (np.argmax(Y,axis=1), np.argmax(y_pred,axis=1)))\n",
    "    accuracy_hls4ml = float(accuracy_score (np.argmax(Y,axis=1), np.argmax(y_pred_hls4ml,axis=1)))\n",
    "\n",
    "    print(\"Accuracy Keras:  {}\".format(accuracy_keras))\n",
    "    print(\"Accuracy hls4ml: {}\".format(accuracy_hls4ml))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(9, 9))\n",
    "    _ = plotting.makeRoc(Y, y_pred, labels=['%i'%nr for nr in range(n_classes)])\n",
    "    plt.gca().set_prop_cycle(None) # reset the colors\n",
    "    _ = plotting.makeRoc(Y, y_pred_hls4ml, labels=['%i'%nr for nr in range(n_classes)], linestyle='--')\n",
    "\n",
    "    from matplotlib.lines import Line2D\n",
    "    lines = [Line2D([0], [0], ls='-'),\n",
    "             Line2D([0], [0], ls='--')]\n",
    "    from matplotlib.legend import Legend\n",
    "    leg = Legend(ax, lines, labels=['Keras', 'hls4ml'],\n",
    "                loc='lower right', frameon=False)\n",
    "    ax.add_artist(leg)\n",
    "    plt.figtext(0.2, 0.38,label, wrap=True, horizontalalignment='left',verticalalignment='center')\n",
    "    plt.ylim(0.01,1.)\n",
    "    plt.xlim(0.7,1.)\n",
    "\n",
    "# Plot the pruned floating point model:    \n",
    "plotROC(Y_test_reduced,y_predict,y_predict_hls4ml,label=\"Keras\") \n",
    "\n",
    "# Plot the pruned and quantized QKeras model\n",
    "plotROC(Y_test_reduced,y_predict_q,y_predict_hls4ml_q,label=\"QKeras\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Looks good! Let's synthesize the models. \n",
    "## Logic synthesis\n",
    "This takes quite a while for CNN models, up to one hour for the models considered here. In the interest of time, we have therefore provided the neccessary reports for the models considered. You can also synthesize them yourself if you have time, and as usual follow the progress using ``tail -f pruned_cnn/vivado_hls.log`` and ``tail -f quantized_pruned_cnn/vivado_hls.log``.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth = False # Only if you want to synthesize the models yourself (>1h per model) rather than look at the provided reports.\n",
    "if synth:\n",
    "    hls_model.build(csim=False, synth=True, vsynth=True)\n",
    "    hls_model_q.build(csim=False, synth=True, vsynth=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We extract the latency from the C synthesis, namely the report in ```<project_dir>/myproject_prj/solution1/syn/report/myproject_csynth.rpt```. A more accurate latency estimate can be obtained from running cosim by passing ```hls_model.build(csim=False, synth=True, vsynth=True, cosim=True)``` ( = C/RTL cosimulation, synthesised HLS code is run on a simulator and tested on C test bench) but this takes a lot of time so we will skip it here.\n",
    "The resource estimates are obtained from the Vivado logic synthesis, and can be extracted from the report in ```<project_dir>/vivado_synth.rpt```. Let's fetch the most relevant numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getReports(indir):\n",
    "    data_ = {}\n",
    "    \n",
    "    report_vsynth = Path('{}/vivado_synth.rpt'.format(indir))\n",
    "    report_csynth = Path('{}/myproject_prj/solution1/syn/report/myproject_csynth.rpt'.format(indir))\n",
    "    \n",
    "    if report_vsynth.is_file() and report_csynth.is_file():\n",
    "        print('Found valid vsynth and synth in {}! Fetching numbers'.format(indir))\n",
    "        \n",
    "        # Get the resources from the logic synthesis report \n",
    "        with report_vsynth.open() as report:\n",
    "            lines = np.array(report.readlines())\n",
    "            data_['lut']     = int(lines[np.array(['CLB LUTs*' in line for line in lines])][0].split('|')[2])\n",
    "            data_['ff']      = int(lines[np.array(['CLB Registers' in line for line in lines])][0].split('|')[2])\n",
    "            data_['bram']    = float(lines[np.array(['Block RAM Tile' in line for line in lines])][0].split('|')[2])\n",
    "            data_['dsp']     = int(lines[np.array(['DSPs' in line for line in lines])][0].split('|')[2])\n",
    "            data_['lut_rel'] = float(lines[np.array(['CLB LUTs*' in line for line in lines])][0].split('|')[5])\n",
    "            data_['ff_rel']  = float(lines[np.array(['CLB Registers' in line for line in lines])][0].split('|')[5])\n",
    "            data_['bram_rel']= float(lines[np.array(['Block RAM Tile' in line for line in lines])][0].split('|')[5])\n",
    "            data_['dsp_rel'] = float(lines[np.array(['DSPs' in line for line in lines])][0].split('|')[5])\n",
    "        \n",
    "        with report_csynth.open() as report:\n",
    "            lines = np.array(report.readlines())\n",
    "            lat_line = lines[np.argwhere(np.array(['Latency (cycles)' in line for line in lines])).flatten()[0] + 3]\n",
    "            data_['latency_clks'] = int(lat_line.split('|')[2])\n",
    "            data_['latency_mus']  = float(lat_line.split('|')[2])*5.0/1000.\n",
    "            data_['latency_ii']   = int(lat_line.split('|')[6])\n",
    "    \n",
    "    return data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pprint \n",
    "\n",
    "data_pruned_ref = getReports('pruned_cnn')\n",
    "data_quantized_pruned = getReports('quantized_pruned_cnn')\n",
    "\n",
    "print(\"\\n Resource usage and latency: Pruned\")\n",
    "pprint.pprint(data_pruned_ref)\n",
    "print(\"\\n Resource usage and latency: Pruned + quantized\")\n",
    "pprint.pprint(data_quantized_pruned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the latency is of around 5 microseconds for both the quantized and the unquantized model, but that the resources are signifcantly reduced using QKeras.\n",
    "\n",
    "Congratulations! You have now reached the end of this notebook. If you have some spare time, you can have a look at the bonus exercise below, where you will learn how to perform a bayesian optimization over the QKeras quantizers in order to obtain an optimally heterogeneously quantized model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hls4ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "3883d520cb54f8a9aa58a406bbec063a51b7ebff2d87db09f120055b14978cef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
